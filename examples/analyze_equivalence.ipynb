{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "import re\n",
    "import string\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from anthropic import Anthropic, HUMAN_PROMPT, AI_PROMPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def normalize_answer(s):\n",
    "    def remove_articles(text):\n",
    "        return re.sub(r'\\b(a|an|the)\\b', ' ', text)\n",
    "\n",
    "    def white_space_fix(text):\n",
    "        return ' '.join(text.split())\n",
    "\n",
    "    def remove_punc(text):\n",
    "        exclude = set(string.punctuation)\n",
    "        return ''.join(ch for ch in text if ch not in exclude)\n",
    "\n",
    "    def lower(text):\n",
    "        return text.lower()\n",
    "\n",
    "    return white_space_fix(remove_articles(remove_punc(lower(s))))\n",
    "\n",
    "def exact_match_score(prediction, ground_truth):\n",
    "    return (normalize_answer(prediction) == normalize_answer(ground_truth))\n",
    "\n",
    "def call_instructgpt_with_answers(questions, model_predictions, ground_truths):\n",
    "    # if exact_match_score(model_prediction, ground_truth):\n",
    "    #     return 1\n",
    "    api_key = open(\"/data/katie_kang/openai_key_file.txt\", \"r\").read()\n",
    "    openai.api_key = api_key.strip()\n",
    "\n",
    "    prompt_template1 = \"\"\"Are the answers equivalent?\n",
    "\n",
    "Ex 1:\n",
    "Q: Who was leader of Revolutionary Army?\n",
    "A1: George Washington\n",
    "A2: US President Washington\n",
    "\n",
    "Ex 2:\n",
    "Q: Who is the screenwriter of Dead Silence?\n",
    "A1: James Wan\n",
    "A2: Leigh Whannell\n",
    "\n",
    "Ex 3:\n",
    "Q: country of citizenship of McKean?\n",
    "A1: United States\n",
    "A2: American\n",
    "\n",
    "Ex 4:\n",
    "Q: What genre is Taxi for Two?\n",
    "A1: romantic comedy film\n",
    "A2: comedy\n",
    "\n",
    "\"\"\" \n",
    "\n",
    "\n",
    "    prompt_template2 = None\n",
    "\n",
    "    for i in range(len(questions)):\n",
    "        if i == 0:\n",
    "            prompt_template2 = \"\"\"Ex {}:\n",
    "Q: {}\n",
    "A1: {}\n",
    "A2: {}\n",
    "\n",
    "\"\"\".format(i+5, questions[i], model_predictions[i], ground_truths[i])\n",
    "        else:\n",
    "            prompt_template2 += \"\"\"Ex {}:\n",
    "Q: {}\n",
    "A1: {}\n",
    "A2: {}\n",
    "\n",
    "\"\"\".format(i+5, questions[i], model_predictions[i], ground_truths[i])\n",
    "\n",
    "    prompt_template3 = \"\"\"Ex 1 Equivalent? Yes\n",
    "Ex 2 Equivalent? No\n",
    "Ex 3 Equivalent? Yes\n",
    "Ex 4 Equivalent? Yes\n",
    "\"\"\"\n",
    "\n",
    "    prompt = prompt_template1 + prompt_template2 + prompt_template3\n",
    "\n",
    "    response = openai.Completion.create(\n",
    "        model=\"gpt-3.5-turbo-instruct\",  # or another model version\n",
    "        # prompt=[filled_prompt, filled_prompt],\n",
    "        prompt=prompt,\n",
    "        max_tokens=200,\n",
    "        temperature=0.0,\n",
    "    )\n",
    "\n",
    "    response = response.choices[0].text.strip()\n",
    "\n",
    "    try:\n",
    "        split_response = response.split(\"\\n\")\n",
    "        split_response = [split_response_i.split(\"? \")[1] for split_response_i in split_response]\n",
    "        linguistically_equivalent = [split_response_i==\"Yes\" for split_response_i in split_response]\n",
    "        assert(len(linguistically_equivalent) == len(questions))\n",
    "    except:\n",
    "        print(response)\n",
    "        linguistically_equivalent = [False for _ in range(len(questions))]\n",
    "\n",
    "\n",
    "    return np.array(linguistically_equivalent)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.load('ckpts/sft_ctrex_llama7B_2_commit_lr1e-5_2/checkpoint_30000/hf_model/output_strings_train.npy')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 581000/581000 [00:06<00:00, 96788.95it/s]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "questions_NEM = []\n",
    "answers_NEM = []\n",
    "predictions_NEM = []\n",
    "exact_match_idxs = []\n",
    "not_exact_match_idxs = []\n",
    "for j in tqdm(range(len(data))):\n",
    "    line = data[j]\n",
    "    answer = line.split('Label: ')[1].strip()\n",
    "    try:\n",
    "        prediction = line.split('The answer is')[1].split('Label:')[0].strip()[0:-1]\n",
    "    except:\n",
    "        prediction = line.split('The Answer Is')[1].split('Label:')[0].strip()[0:-1]\n",
    "    question = line.split('<unk>')[-1].split('?')[0].strip() + '?'\n",
    "\n",
    "    if exact_match_score(answer, prediction):\n",
    "        exact_match_idxs.append(j)\n",
    "    else:\n",
    "        questions_NEM.append(question)\n",
    "        answers_NEM.append(answer)\n",
    "        predictions_NEM.append(prediction)\n",
    "        not_exact_match_idxs.append(j)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "questions_NEM = np.array(questions_NEM)\n",
    "answers_NEM = np.array(answers_NEM)\n",
    "predictions_NEM = np.array(predictions_NEM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/10 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2154\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "ZeroDivisionError",
     "evalue": "division by zero",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mZeroDivisionError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[75], line 7\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# for i in tqdm(range(0, len(not_exact_match_idxs)//num_points_per_prompt)):\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m tqdm(\u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m10\u001b[39m)):\n\u001b[0;32m----> 7\u001b[0m     results \u001b[38;5;241m=\u001b[39m \u001b[43mcall_instructgpt_with_answers\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquestions_NEM\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mnum_points_per_prompt\u001b[49m\u001b[43m:\u001b[49m\u001b[43m(\u001b[49m\u001b[43mi\u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mnum_points_per_prompt\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswers_NEM\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mnum_points_per_prompt\u001b[49m\u001b[43m:\u001b[49m\u001b[43m(\u001b[49m\u001b[43mi\u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mnum_points_per_prompt\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpredictions_NEM\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mnum_points_per_prompt\u001b[49m\u001b[43m:\u001b[49m\u001b[43m(\u001b[49m\u001b[43mi\u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mnum_points_per_prompt\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     11\u001b[0m     points_in_consideration \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marange(i\u001b[38;5;241m*\u001b[39mnum_points_per_prompt, (i\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m*\u001b[39mnum_points_per_prompt)\n\u001b[1;32m     12\u001b[0m     linguistically_equivalent_idxs\u001b[38;5;241m.\u001b[39mappend(points_in_consideration[np\u001b[38;5;241m.\u001b[39mwhere(results \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m)[\u001b[38;5;241m0\u001b[39m]])\n",
      "Cell \u001b[0;32mIn[74], line 78\u001b[0m, in \u001b[0;36mcall_instructgpt_with_answers\u001b[0;34m(questions, model_predictions, ground_truths)\u001b[0m\n\u001b[1;32m     75\u001b[0m prompt \u001b[38;5;241m=\u001b[39m prompt_template1 \u001b[38;5;241m+\u001b[39m prompt_template2 \u001b[38;5;241m+\u001b[39m prompt_template3\n\u001b[1;32m     77\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28mlen\u001b[39m(prompt))\n\u001b[0;32m---> 78\u001b[0m \u001b[38;5;241;43m1\u001b[39;49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\n\u001b[1;32m     80\u001b[0m response \u001b[38;5;241m=\u001b[39m openai\u001b[38;5;241m.\u001b[39mCompletion\u001b[38;5;241m.\u001b[39mcreate(\n\u001b[1;32m     81\u001b[0m     model\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgpt-3.5-turbo-instruct\u001b[39m\u001b[38;5;124m\"\u001b[39m,  \u001b[38;5;66;03m# or another model version\u001b[39;00m\n\u001b[1;32m     82\u001b[0m     \u001b[38;5;66;03m# prompt=[filled_prompt, filled_prompt],\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     85\u001b[0m     temperature\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.0\u001b[39m,\n\u001b[1;32m     86\u001b[0m )\n\u001b[1;32m     88\u001b[0m response \u001b[38;5;241m=\u001b[39m response\u001b[38;5;241m.\u001b[39mchoices[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mtext\u001b[38;5;241m.\u001b[39mstrip()\n",
      "\u001b[0;31mZeroDivisionError\u001b[0m: division by zero"
     ]
    }
   ],
   "source": [
    "linguistically_equivalent_idxs = []\n",
    "not_linguistically_equivalent_idxs = []\n",
    "\n",
    "num_points_per_prompt = 20\n",
    "# for i in tqdm(range(0, len(not_exact_match_idxs)//num_points_per_prompt)):\n",
    "for i in tqdm(range(10)):\n",
    "    results = call_instructgpt_with_answers(questions_NEM[i*num_points_per_prompt:(i+1)*num_points_per_prompt], \n",
    "    answers_NEM[i*num_points_per_prompt:(i+1)*num_points_per_prompt]\n",
    "    , predictions_NEM[i*num_points_per_prompt:(i+1)*num_points_per_prompt])\n",
    "\n",
    "    points_in_consideration = np.arange(i*num_points_per_prompt, (i+1)*num_points_per_prompt)\n",
    "    linguistically_equivalent_idxs.append(points_in_consideration[np.where(results == 1)[0]])\n",
    "    not_linguistically_equivalent_idxs.append(points_in_consideration[np.where(results == 0)[0]])\n",
    "\n",
    "    if i%2 == True:\n",
    "        equivalence = np.ones(len(data))*-1\n",
    "        equivalence[np.array(exact_match_idxs)] = 1\n",
    "        equivalence[np.concatenate(linguistically_equivalent_idxs)] = 1\n",
    "        equivalence[np.concatenate(not_linguistically_equivalent_idxs)] = 0\n",
    "        np.save(equivalence, \"ckpts/sft_ctrex_llama7B_2_commit_lr1e-5_2/checkpoint_30000/hf_model/output_strings_train_linguistic_equivalence.npy\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.18 ('trlx')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  },
  "vscode": {
   "interpreter": {
    "hash": "72a3fd4a13b86e2ebaa0e8e208d83864f5fab883245b2c176f0efba32410842e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
